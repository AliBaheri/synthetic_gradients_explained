{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are synthetic gradients?\n",
    "\n",
    "### Demo - We're going to use a newer optimization strategy called \"Synthetic Gradients\" instead of \"Backpropagation\" to train our simple feedforward Neural Network.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms/documents/3-6.gif\">\n",
    "\n",
    "## How do Neural Networks Learn?\n",
    "\n",
    "![alt text](http://datathings.com/blog/images/neuralnet/nnblackbox.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.intechopen.com/source/html/38738/media/f2.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Learning process\n",
    "- Use inputs + desired outputs to update internal state accordingly\n",
    "\n",
    "Prediction process \n",
    "- Use input and internal state to generate most likely output according to its past “training experience”\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-b2afcc88428418db01552987182e7b6a.webp \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-7bdfcff266211a74a31bfcdcc99c0087.webp \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "![alt text](http://datathings.com/blog/images/neuralnet/derivative2.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:99 Loss:898.459296751\n",
      "Iter:199 Loss:235.789405208\n",
      "Iter:299 Loss:33.1476779493\n",
      "Iter:399 Loss:11.4531992859\n",
      "Iter:499 Loss:6.59559541298\n",
      "Iter:523 Loss:5.87630756375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ff293d77733a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mlayer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mlayer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mlayer_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ff293d77733a>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_output_delta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    def int2vec(x,dim=output_dim):\n",
    "        out = np.zeros(dim)\n",
    "        binrep = np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out[-len(binrep):] = binrep\n",
    "        return out\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "\n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_out2deriv(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "class DNI(object):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv):\n",
    "        \n",
    "        #same as before\n",
    "        self.weights = (np.random.randn(input_dim, output_dim) * 0.2) - 0.1\n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_deriv = nonlin_deriv\n",
    "        \n",
    "        \n",
    "        # new stuff\n",
    "        self.weights_synthetic_grads = (np.random.rand(output_dim,output_dim) * 0.2) -0.1\n",
    "        self.alpha = alpha \n",
    "    \n",
    "    # used to be just \"forward\", but now we update during the forward pass using Synthetic Gradients :)\n",
    "    def forward_and_synthetic_update(self,input):\n",
    "\n",
    "    \t# cache input\n",
    "        self.input = input\n",
    "\n",
    "        # forward propagate\n",
    "        self.output = self.nonlin(self.input.dot(self.weights))\n",
    "        \n",
    "        # generate synthetic gradient via simple linear transformation\n",
    "        self.synthetic_gradient = self.output.dot(self.weights_synthetic_grads)\n",
    "\n",
    "        # update our regular weights using synthetic gradient\n",
    "        self.weight_synthetic_gradient = self.synthetic_gradient * self.nonlin_deriv(self.output)\n",
    "        self.weights += self.input.T.dot(self.weight_synthetic_gradient) * self.alpha\n",
    "        \n",
    "        # return backpropagated synthetic gradient (this is like the output of \"backprop\" method from the Layer class)\n",
    "        # also return forward propagated output (feels weird i know... )\n",
    "        return self.weight_synthetic_gradient.dot(self.weights.T), self.output\n",
    "    \n",
    "        \n",
    "    # this is just like the \"update\" method from before... except it operates on the synthetic weights\n",
    "    def update_synthetic_weights(self,true_gradient):\n",
    "        self.synthetic_gradient_delta = self.synthetic_gradient - true_gradient \n",
    "        self.weights_synthetic_grads += self.output.T.dot(self.synthetic_gradient_delta) * self.alpha\n",
    "        \n",
    "\n",
    "    \n",
    "np.random.seed(1)\n",
    "\n",
    "num_examples = 1000\n",
    "output_dim = 12\n",
    "iterations = 1000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 10\n",
    "alpha = 0.1\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 128\n",
    "layer_2_dim = 64\n",
    "output_dim = len(y[0])\n",
    "\n",
    "layer_1 = DNI(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_2 = DNI(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_3 = DNI(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv,alpha)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "\n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]  \n",
    "        \n",
    "      _, layer_1_out = layer_1.forward_and_synthetic_update(batch_x)\n",
    "        layer_1_delta, layer_2_out = layer_2.forward_and_synthetic_update(layer_1_out)\n",
    "        layer_3_out = layer_3.forward_and_synthetic_update(layer_2_out,False)\n",
    "\n",
    "        layer_3_delta = layer_3_out - batch_y\n",
    "        layer_2_delta = layer_3.normal_update(layer_3_delta)\n",
    "        layer_2.update_synthetic_weights(layer_2_delta)\n",
    "        layer_1.update_synthetic_weights(layer_1_delta)\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta)))\n",
    "        synthetic_error += (np.sum(np.abs(layer_2_delta - layer_2.synthetic_gradient)))\n",
    "    if(iter % 100 == 99):\n",
    "        sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error) + \" Synthetic Loss:\" + str(synthetic_error))\n",
    "    if(iter % 10000 == 9999):\n",
    "        print(\"\")\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta * layer_3_out * (1 - layer_3_out))))\n",
    "\n",
    "    sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error))\n",
    "    if(iter % 100 == 99):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem with Backpropagation\n",
    "\n",
    "### Locking\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/3-1.width-1500_zU6x0wC.png \"Logo Title Text 1\")\n",
    "\n",
    "- A layer can only be updated after a full forward+backward pass been \n",
    "- After Layer 1 has processed input, it updates after output activations (black lines) have been propagated through the rest of the network, generated a loss, and the error gradients (green lines) backpropagated through every layer until Layer 1 is reached. \n",
    "- So L1 must wait for forward+backward pass of L2 & L3 before updating\n",
    "- Therefore L1 is locked/coupled to the rest of the network\n",
    "\n",
    "![alt text](https://www.semiwiki.com/forum/attachments/content/attachments/17619d1467046829-googlenet-inceptions-jpg \"Logo Title Text 1\")\n",
    "\n",
    "- For simple networks it's a non-issue\n",
    "- But consider a complex system of multiple networks, acting in multiple environments at asynchronous and irregular timescales.\n",
    "-  Or a big distributed network spread over multiple machines. Time expensive\n",
    "\n",
    "\n",
    "### If we decouple the interfaces - the connections -  between layers, every layer can be updated independently, and is not locked to the rest of the network. But how?\n",
    "\n",
    "\n",
    "## Synthetic Gradients\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/3-3.width-1500_Ij679hz.png \"Logo Title Text 1\")\n",
    "\n",
    "- Normally, a neural network compares its predictions to a dataset to decide how to update its weights. \n",
    "- It then uses backpropagation to figure out how each weight should move in order to make the prediction more accurate. \n",
    "- However, with Synthetic Gradients, individual layers instead make a \"best guess\" for what they think the data will say, and then update their weights according to this guess. \n",
    "- This \"best guess\" is called a Synthetic Gradient. \n",
    "- The data is only used to help update each layer's \"guesser\" or Synthetic Gradient generator. \n",
    "- This allows for (most of the time), individual layers to learn in isolation, which increases the speed of training.\n",
    "\n",
    "If we use a synthetic gradient model we can do the following:\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/3-4.width-1500_jjNNlb7.png \"Logo Title Text 1\")\n",
    "\n",
    "... and use the synthetic gradients (blue) to update Layer 1 before the rest of the network has even been executed.\n",
    "\n",
    "The synthetic gradient model itself is trained to regress target gradients - these target gradients could be the true gradients backpropagated from the loss or other synthetic gradients which have been backpropagated from a further downstream synthetic gradient model.\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/3-5.width-1500_pmWHi94.png \"Logo Title Text 1\")\n",
    "\n",
    "Animated:\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/documents/3-6.gif \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/synthetic_grads_paper.png \"Logo Title Text 1\")\n",
    "\n",
    "- Synthetic Gradient generators are just neural nets trained to take the output of a layer and predict the gradient that will likely happen at that layer.\n",
    "- When we perform full forward + back pass, we get the \"correct\" gradient\n",
    "- We can compare this to our \"synthetic\" gradient \n",
    "- So we can train our Synthetic Gradient networks by pretending that our \"true gradients\" are coming from from mythical dataset\n",
    "\n",
    "See how the gradient (M i+2) backpropagates through (f i+1) and into M(i+1)? As you can see, each synthetic gradient generator is actually only trained using the Synthetic Gradients generated from the next layer. Thus, only the last layer actually trains on the data. All the other layers, including the Synthetic Gradient generator networks, train based on Synthetic Gradients. Thus, the network can train with each layer only having to wait on the synthetic gradient from the following layer (which has no other dependencies). \n",
    "\n",
    "- DNI doesn’t magically allow networks to train without true gradient information. The true gradient information does percolate backwards through the network, but just slower and over many training iterations, through the losses of the synthetic gradient models. \n",
    "- But overall the network is faster  because the synthetic gradient models approximate and smooth over the absence of true gradients.\n",
    "- DNI can be applied to any generic neural network architecture, not just feed-forward networks\n",
    "- This is awesome! I want to see this integrated into all major DL libraries. allow distributed training of networks + faster + cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
